@testset "Gradient descent" begin
    using Statistics
    using LinearAlgebra

    mutable struct Decay i end
    Base.:*(Î´Î·::Decay, x) = x/sqrt(Î´Î·.i+=1)

    loss_squared(x, y, ğ°, Ï†) = (ğ°â‹…Ï†(x) - y)^2
    mean_loss(ğ°, ğ’Ÿtrain, Ï†, loss) = mean(loss(x, y, ğ°, Ï†) for (x,y) âˆˆ ğ’Ÿtrain)

    """
    Single-dimensional training input data.
    """
    function test_gradient_descent()
        ğ’Ÿtrain = [(3,4), (-1,3), (-1,0)]
        ğ°_opt = gradient_descent(ğ’Ÿtrain, x->x, âˆ‡loss_squared)
        y_opt = mean_loss(ğ°_opt, ğ’Ÿtrain, x->x, loss_squared)
        return (ğ°_opt, y_opt)
    end

    """
    Decay learning rate Î·.
    """
    function test_gradient_descent_decay(T)
        ğ’Ÿtrain = [(3,4), (-1,3), (-1,0)]
        ğ°_opt = gradient_descent(ğ’Ÿtrain, x->x, âˆ‡loss_squared; Î·=Decay(0), T=T)
        y_opt = mean_loss(ğ°_opt, ğ’Ÿtrain, x->x, loss_squared)
        return (ğ°_opt, y_opt)
    end

    """
    Multi-dimensional training data input.
    """
    function test_gradient_descent_multi()
        ğ’Ÿtrain = [([3,0.7],4), ([-1,0.3],3), ([-1,-3],0)]
        ğ°_opt = gradient_descent(ğ’Ÿtrain, x->x, âˆ‡loss_squared)
        y_opt = mean_loss(ğ°_opt, ğ’Ÿtrain, x->x, loss_squared)
        return (ğ°_opt, y_opt)
    end

    ğ°, y = test_gradient_descent()
    @test ğ° â‰ˆ [0.8181818181818182]
    @test y â‰ˆ 5.878787878787879

    ğ°, y = test_gradient_descent_decay(30)
    @test ğ° â‰ˆ [0.41794205540127405]
    @test y â‰ˆ 6.466158060393507

    ğ°, y = test_gradient_descent_multi()
    @test ğ° â‰ˆ [0.8314306533883896, -0.03036191401505953]
    @test y â‰ˆ 5.876487733786738
end
